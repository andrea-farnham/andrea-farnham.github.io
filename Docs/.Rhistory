shinylive:::quarto_ext()
install.packages(shinylive)
quarto::quarto_preview()
quarto::quarto_preview()
quarto::quarto_preview()
quarto::quarto_preview()
quarto::quarto_preview()
quarto_render()
quarto::quarto_render()
quarto::quarto_preiew()
quarto::quarto_preview()
quarto::quarto_render()
quarto::quarto_preview(
)
getwd()
quarto::quarto_preview(
)
quarto::quarto_preview(
()
quarto::quarto_preview()
quarto::quarto_preview()
quarto::quarto_preview()
quarto::quarto_render()
quarto::quarto_preview()
quarto::quarto_render()
quarto::quarto_render()
quarto::quarto_render()
AFFORD <- data.frame(
Topic = c(
NA,
"Metadata", "Metadata","Metadata",
"Documentation", "Documentation", "Documentation", "Documentation",
"Code", "Code"
),
Output = c(
"Metadata",
"Tables", "README", "JSON",
"Protocols", "Filenaming", "Folder Organization","JSON fields",
"R-Quarto", ".gitlab-ci.yml"
),
Title = c(
"Metadata",
"TablesS", "README", "JSON",
"Protocols", "Filenaming", "Folder Organization","JSON fields",
"R-Quarto", ".gitlab-ci.yml"
)
)
library(collapsibleTree)
collapsibleTree(AFFORD, c("Topic", "Output"),  fill = "Title",
collapsed = FALSE,  attribute = "Title")
View(AFFORD)
getwd()
quarto::quarto_render()
quarto::quarto_preview()
quarto::quarto_preview()
quarto::quarto_preview()
# Install and load the required packages
install.packages("rvest")
install.packages("httr")
library(rvest)
library(httr)
# Specify the URL of the website you want to scrape
url <- "https://example.com"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- content(response, "text/html")
# Install and load the required packages
install.packages("rvest")
install.packages("httr")
library(rvest)
library(httr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- content(response, "text/html")
install.packages("rvest")
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
library(rvest)
library(httr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/"
#https://www.crs.uzh.ch/en/resources/CRS-Primers.html
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- content(response, "text/html")
html_content <- content(response, "text")
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- content(response, "text")
# Use rvest to extract all the links from the HTML content
links <- html_nodes(html_content, "a") %>%
html_attr("href")
response
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
response
html_nodes(html_content)
html_nodes(html_content,'text/html')
html_content <- content(response, "text/html")
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- content(response, "text/html")
html_content <- read_html(content(response, "text"))
html_content
# Use rvest to extract all the links from the HTML content
links <- html_nodes(html_content, "a") %>%
html_attr("href")
links
# Use rvest to extract all the links from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links)
)
title
links_data
links_data[1]
links_data[12]
links_data[1,2]
links_data[1,1]
links_data$link[1]
links_data[1]$link
links_data$link[1]
links_data$link[1], links_data$title[1]
c(links_data$link[1], links_data$title[1])
c(links_data$link[10], links_data$title[10])
c(links_data$link[2], links_data$title[2])
lins_data$link
linsk_data$link
links_data$link
links_data$link[19]
links_data$title[19]
links_data$link[20]
links_data$title[20]
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links,"title")
)
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links,"title")
)
links_data$title[20]
links
View(links)
str(links)
links[1]
links[[1]]
links[[2]]
links[[19]]
html_nodes(links)
html_text(html_nodes(links,".title-class")
)
links_data <- data.frame(
link = html_attr(links, "href"),
#title = html_text(links,"title")
title = html_text(html_nodes(links, ".title-class"))
)
# Use rvest to extract all the links from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
#title = html_text(links,"title")
title = html_text(html_nodes(links, ".title-class"))
)
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links,"title")
)
html_text(links)
html_attr(links)
html_attr(links,".title-class")
html_attr(links,"title")
html_attr(links,"id")
html_attr(links,"title")
# Use rvest to extract all the links from the HTML content
links <- html_nodes(html_content, "a")
html_text(links,"title")
html_text(links,"id")
html_attr(links, "href")
html_attr(links, "src")
html_attr(links, "title")
html_attr(links, "href")
html_nodes(html_content, "a")
html_nodes(html_content, "title")
html_nodes(html_content, ".title")
html_nodes(html_content, "a")
links[20]
x = links[20]
View(x)
x[[1]]
str(x)
x[[1]]
x[[1]]$node
x[[1]]$doc
x[[2]]$doc
x[[1]]$node
x[[1]]$doc
# Use rvest to extract all the links from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links,"title")
)
links
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(html_nodes(links, ".title-class"))
)
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(html_nodes(links, "link"))
)
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_attr(links, "title")
)
# Filter out only the links belonging to doi.org
doi_links <- subset(links_data, str_detect(link, "doi.org"))
# Print the data frame with links and titles belonging to doi.org
print(doi_links)
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links, "title")
)
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links, "title")
)
# Filter out only the links belonging to doi.org
doi_links <- subset(links_data, str_detect(link, "doi.org"))
# Print the data frame with links and titles belonging to doi.org
print(doi_links)
print(doi_links)
str_replace_all(doi_links$title, "\\s| More about", "")
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links, "title")
)
# Filter out only the links belonging to doi.org
doi_links <- subset(links_data, str_detect(link, "doi.org"))
# Print the data frame with links and titles belonging to doi.org
#print(doi_links)
doi_links$title <- str_replace_all(doi_links$title, "\\s| More about", "")
print(doi_links)
str_replace_all(doi_links$title, "More about", "")
str_replace_all(doi_links$title, "Moreabout", "")
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links, "title")
)
# Filter out only the links belonging to doi.org
doi_links <- subset(links_data, str_detect(link, "doi.org"))
# Print the data frame with links and titles belonging to doi.org
#print(doi_links)
doi_links$title <- str_replace_all(doi_links$title, "Moreabout", "")
print(doi_links)
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links, "title")
)
# Filter out only the links belonging to doi.org
doi_links <- subset(links_data, str_detect(link, "doi.org"))
# Print the data frame with links and titles belonging to doi.org
#print(doi_links)
doi_links$title <- str_replace_all(doi_links$title, "More about", "")
print(doi_links)
# Install and load the required packages
#install.packages("rvest")
#install.packages("httr")
#install.packages("stringr")
library(rvest)
library(httr)
library(stringr)
# Specify the URL of the website you want to scrape
url <- "https://www.crs.uzh.ch/en/resources/CRS-Primers.html"
# Send an HTTP GET request to the URL and read the HTML content
response <- GET(url)
html_content <- read_html(content(response, "text"))
# Use rvest to extract all the links and their associated titles from the HTML content
links <- html_nodes(html_content, "a")
links_data <- data.frame(
link = html_attr(links, "href"),
title = html_text(links, "title")
)
# Filter out only the links belonging to doi.org
doi_links <- subset(links_data, str_detect(link, "doi.org"))
# Print the data frame with links and titles belonging to doi.org
#print(doi_links)
doi_links$title <- str_replace_all(doi_links$title, "\\s|More about", "")
print(doi_links)
quarto::quarto::quarto_render()
quarto::quarto_render()
